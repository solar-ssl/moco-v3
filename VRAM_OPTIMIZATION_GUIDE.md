# VRAM Optimization Guide

## Problem Discovered

Your training is only using **4.6 GB out of 8 GB VRAM** (58% utilization) - **wasting 42% of your GPU memory!**

This happened because `config_low_vram.py` was designed conservatively for worst-case scenarios. Your actual VRAM usage is much lower.

---

## Solution: Switch to Optimized Config

### Option 1: Maximum Performance (Recommended for 2000 images)

**Use:** `config_optimized_16gb.py`

```python
# In your training script, change:
from src.config_low_vram import Config
# to:
from src.config_optimized_16gb import Config
```

**Benefits:**
- ‚úÖ Batch size: 64 ‚Üí 96 (+50%)
- ‚úÖ Queue size: 65,536 ‚Üí 76,800 (divisible by batch!)
- ‚úÖ Effective batch: 512 ‚Üí 768 (+50%)
- ‚úÖ Epochs: 300 ‚Üí 600 (better for small datasets)
- ‚úÖ VRAM usage: 4.6 GB ‚Üí 6.9 GB (86% utilization)
- ‚úÖ Training speed: ~35% faster per epoch
- ‚úÖ Total training time: 15 hours (was 10 hrs but 2√ó epochs)

**Safe?** YES! Estimated 6.9 GB with 1.1 GB safety margin.

**Queue compatibility:** 76,800 / 96 = 800 batches (perfect division) ‚úÖ

---

### Option 2: Even Larger Effective Batch (Conservative)

If you want to prioritize safety or maximize effective batch:

**Modify `config_low_vram.py`:**
```python
batch_size: int = 80  # Instead of 64
queue_size: int = 64000  # MUST change for compatibility! (80 √ó 800)
# Keep grad_accum=4
# Effective batch = 80 * 2 * 4 = 640
# VRAM usage: ~5.75 GB
```

**Benefits:**
- ‚úÖ 25% larger batch with minimal risk
- ‚úÖ Effective batch: 512 ‚Üí 640
- ‚úÖ Still very safe (~2.25 GB margin)

**‚ö†Ô∏è IMPORTANT:** Queue size MUST be divisible by batch size! 64,000 / 80 = 800 ‚úÖ

---

### Option 3: Aggressive (If you want to push limits)

**Test incrementally:**
```python
batch_size: int = 128  # Risky - may OOM!
queue_size: int = 65536  # Keeps original queue (128 is power of 2)
# Estimated VRAM: ~9.2 GB (LIKELY TOO HIGH!)
# Effective batch = 128 * 2 * 4 = 1024
```

‚ö†Ô∏è **Only try this if:**
- You can monitor VRAM usage in real-time
- You're okay with potential OOM crashes
- You want maximum batch size for experiments
- **Note:** batch=128 is divisible into 65536, but VRAM will likely exceed 8GB!

---

## Quick Start

### Step 1: Copy optimized config to training machine
```bash
# Copy config_optimized_16gb.py to your GPU machine
scp src/config_optimized_16gb.py user@gpu-machine:~/moco-v3/src/
```

### Step 2: Update training script import
```bash
# On your GPU machine
sed -i 's/from src.config_low_vram import Config/from src.config_optimized_16gb import Config/' src/training/train_moco.py
```

### Step 3: Restart training
```bash
python main.py --multiprocessing-distributed --world-size 1 --rank 0
```

### Step 4: Monitor first epoch
```bash
# Watch VRAM usage (should peak around 6.9 GB)
nvidia-smi -l 1
```

If VRAM exceeds 7.5 GB ‚Üí Reduce batch_size to 88 or 80

---

## Why Was Previous Config Using 8GB?

Possible reasons your earlier training used full 8GB:

1. **Different model:** ViT-Base (5GB) instead of ResNet-50 (2.5GB)
2. **Larger batch:** You might have manually increased batch size
3. **No AMP:** Mixed precision disabled (uses 2√ó VRAM)
4. **No grad checkpointing:** Disabled (uses 30% more VRAM)
5. **Different dataset:** Larger images or different preprocessing

---

## Comparison Table

| Config | Batch | Queue | Eff. Batch | VRAM | Util. | Epochs | Time | Best For | Compatible |
|--------|-------|-------|------------|------|-------|--------|------|----------|------------|
| `config_low_vram.py` (old) | 64 | 65,536 | 512 | 4.6 GB | 58% | 300 | 10h | Conservative | ‚úÖ |
| `config_optimized_16gb.py` ‚≠ê | 96 | 76,800 | 768 | 6.9 GB | 86% | 600 | 15h | **2000 images** | ‚úÖ |
| Manual batch=80 | 80 | 64,000 | 640 | 5.8 GB | 72% | 300 | 8h | Safe upgrade | ‚úÖ |
| Aggressive batch=128 | 128 | 65,536 | 1024 | 9.2 GB | 115% | 300 | 7h | Won't fit! | ‚úÖ (but OOM) |

**Note:** "Compatible" means `queue_size % batch_size == 0`. All configs shown have compatible queue/batch settings.

---

## Dataset Size Considerations

With only **2,000 images**, you need to compensate for small dataset size:

### Recommended Settings:
- ‚úÖ **Epochs: 600-1000** (vs 300 in paper)
  - Paper: 1.2M images √ó 300 epochs = 360M views
  - You: 2k images √ó 600 epochs = 1.2M views (300√ó less, but acceptable)

- ‚úÖ **Larger effective batch: 768+** (via bigger batch or more accumulation)
  - More negatives per update = better representations
  - Queue (65k) becomes CRITICAL with small datasets

- ‚úÖ **Strong augmentations** (already in `fix/satellite-augmentations`)
  - Discrete rotations, vertical flips, noise
  - Creates more diversity from limited data

---

## Expected Performance

With 2,000 images and optimized config:

| Metric | Paper (1.2M imgs) | You (2k imgs, optimized) | You (2k imgs, old config) |
|--------|-------------------|--------------------------|---------------------------|
| Linear probe mIoU | ~73% | ~62-67% | ~58-63% |
| Fine-tune mIoU | ~85% | ~74-79% | ~70-75% |
| Training time | 6 days (8√óV100) | 15 hours (2√ó8GB) | 10 hours |
| Total views | 360M | 1.2M | 600k |

Still significantly better than random initialization! üéâ

---

## Troubleshooting

### "CUDA out of memory" error
```bash
# Reduce batch size incrementally:
batch_size = 88  # Try this first
batch_size = 80  # If still OOM
batch_size = 72  # Safe fallback
```

### VRAM usage lower than expected
```bash
# Check if AMP is actually enabled:
python -c "from src.config_optimized_16gb import Config; print(f'AMP: {Config.use_amp}')"

# Should print: AMP: True
```

### Want even more VRAM optimization
```python
# In config, try:
use_grad_checkpointing = True  # Should already be True
batch_size = 104  # Push higher (risky!)
gradient_accumulation_steps = 2  # Reduce accumulation (faster but smaller eff. batch)
```

---

## Migration Path

If you've already started training with old config:

### Continue from checkpoint with new config:
```python
# The optimizer state will adjust automatically
# Only potential issue: learning rate schedule might be slightly off
# Solution: Either start fresh OR accept minor LR discrepancy
```

### Recommended:
Since you're early in training (only seeing the issue now), **start fresh with optimized config** for best results.

---

## Summary

üéØ **Action:** Switch to `config_optimized_16gb.py` immediately

**Why:**
- You're wasting 42% of your VRAM
- Training could be 35% faster per epoch
- Better gradients from larger batches
- Longer training (600 epochs) compensates for small dataset

**Risk:** Low (6.9 GB estimate with 1.1 GB margin)

**Expected improvement:**
- 5-10% better final performance
- Faster iteration for experiments
- More efficient use of your hardware

Start using what you have! üöÄ

---

## ‚ö†Ô∏è CRITICAL: Queue/Batch Compatibility

### Why Queue Size Matters

MoCo uses a **queue** to store negative samples. The queue is updated by **replacing the oldest batch** with the current batch. This requires:

```
queue_size % batch_size == 0
```

**If not divisible:**
- Queue pointer arithmetic breaks
- Index out of bounds errors
- Training crashes or corrupts queue

### Valid Configurations

| Batch Size | Queue Size | Divisible? | Queue Batches |
|------------|------------|------------|---------------|
| 64 | 65,536 | ‚úÖ YES | 1,024 |
| 96 | 76,800 | ‚úÖ YES | 800 |
| 80 | 64,000 | ‚úÖ YES | 800 |
| 128 | 65,536 | ‚úÖ YES | 512 |
| **96** | **65,536** | ‚ùå **NO** | **682.67 (broken!)** |

### How to Fix Incompatible Configs

If you change `batch_size`, you **MUST** update `queue_size`:

**Method 1: Calculate new queue_size**
```python
batch_size = 96
target_batches = 800  # How many batches you want in queue
queue_size = batch_size * target_batches  # 96 √ó 800 = 76,800
```

**Method 2: Use power-of-2 batch sizes**
Powers of 2 (32, 64, 128, 256) are always divisible into 65,536:
```python
batch_size = 64  # or 128
queue_size = 65536  # No change needed
```

### Checking Your Config

```python
from src.config_optimized_16gb import Config
c = Config()

# Must be True!
assert c.queue_size % c.batch_size == 0, "Incompatible queue/batch size!"
print(f"‚úÖ Queue holds {c.queue_size // c.batch_size} batches")
```

---
